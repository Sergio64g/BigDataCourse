{
  "metadata": {
    "name": "NotebookPyspatk",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nnums\u003d sc.parallelize([1,2,3,4])"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\r\nnums.take(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nsquared \u003d nums.map(lambda x: x*x).collect()\nfor num in squared:\n    print(\u0027%i \u0027 % (num))"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql import Row\nfrom pyspark.sql import SQLContext\nsqlContext \u003d SQLContext(sc)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nlist_p \u003d [(\u0027John\u0027,19),(\u0027Smith\u0027,29),(\u0027Adam\u0027,35),(\u0027Henry\u0027,50)]\nrdd \u003d sc.parallelize(list_p)\nppl \u003d rdd.map(lambda x: Row(name\u003dx[0], age\u003dint(x[1])))\nDF_ppl \u003d sqlContext.createDataFrame(ppl)"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n#from pyspark.sql import SQLContext\nurl \u003d \"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/adult_data.csv\"\nfrom pyspark import SparkFiles\nsc.addFile(url)\nsqlContext \u003d SQLContext(sc)"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nadults \u003d \"s3://bucketsgr/adult_data.csv\"\ndf \u003d sqlContext.read.csv(adults, header\u003dTrue, inferSchema\u003d True)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.show(5, truncate \u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_string \u003d sqlContext.read.csv(adults, header\u003dTrue, inferSchema\u003d False)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_string.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Import all from `sql.types`\nfrom pyspark.sql.types import *\n# Write a custom function to convert the data type of DataFrame columns\ndef convertColumn(df, names, newType):\n    for name in names: \n        df \u003d df.withColumn(name, df[name].cast(newType))\n    return df \n# List of continuous features\nCONTI_FEATURES  \u003d [\u0027age\u0027, \u0027fnlwgt\u0027,\u0027capital-gain\u0027, \u0027educational-num\u0027, \u0027capital-loss\u0027, \u0027hours-per-week\u0027]\n# Convert the type\ndf_string \u003d convertColumn(df_string, CONTI_FEATURES, FloatType())\n# Check the dataset\ndf_string.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.select(\u0027age\u0027,\u0027fnlwgt\u0027).show(5)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.describe().show()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.describe(\u0027capital-gain\u0027).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.drop(\u0027education-num\u0027).columns"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.filter(df.age \u003e 40).count()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.groupby(\u0027marital-status\u0027).agg({\u0027capital-gain\u0027: \u0027mean\u0027}).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.sql.functions import *\n# 1 Select the column\nage_square \u003d df.select(col(\"age\")**2)\n# 2 Apply the transformation and add it to the DataFrame\ndf \u003d df.withColumn(\"age_square\", col(\"age\")**2)\ndf.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nCOLUMNS \u003d [\u0027age\u0027, \u0027age_square\u0027, \u0027workclass\u0027, \u0027fnlwgt\u0027, \u0027education\u0027, \u0027educational-num\u0027, \u0027marital-status\u0027,\n           \u0027occupation\u0027, \u0027relationship\u0027, \u0027race\u0027, \u0027gender\u0027, \u0027capital-gain\u0027, \u0027capital-loss\u0027,\n           \u0027hours-per-week\u0027, \u0027native-country\u0027, \u0027income\u0027]\ndf \u003d df.select(COLUMNS)\ndf.first()"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf.filter(df[\u0027native-country\u0027] \u003d\u003d \u0027Holand-Netherlands\u0027).count()\ndf.groupby(\u0027native-country\u0027).agg({\u0027native-country\u0027: \u0027count\u0027}).sort(asc(\"count(native-country)\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_remove \u003d df.filter(df[\u0027native-country\u0027] !\u003d\t\u0027Holand-Netherlands\u0027)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\nstringIndexer \u003d StringIndexer(inputCol\u003d\"workclass\", outputCol\u003d\"workclass_encoded\")\nmodel \u003d stringIndexer.fit(df)\nindexed \u003d model.transform(df)\nencoder \u003d OneHotEncoder(dropLast\u003dFalse, inputCol\u003d\"workclass_encoded\", outputCol\u003d\"workclass_vec\")\nencoded \u003d encoder.transform(indexed)\nencoded.show(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoderEstimator\nCATE_FEATURES \u003d [\u0027workclass\u0027, \u0027education\u0027, \u0027marital-status\u0027, \n\u0027occupation\u0027, \u0027relationship\u0027, \u0027race\u0027, \u0027gender\u0027, \u0027native-country\u0027]\nstages \u003d [] # stages in our Pipeline\nfor categoricalCol in CATE_FEATURES:\n    stringIndexer \u003d StringIndexer(inputCol\u003dcategoricalCol, outputCol\u003dcategoricalCol + \"Index\")\n    encoder \u003d OneHotEncoderEstimator(inputCols\u003d[stringIndexer.getOutputCol()],\n                                     outputCols\u003d[categoricalCol + \"classVec\"])\n    stages +\u003d [stringIndexer, encoder]"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Convert label into label indices using the StringIndexer\nlabel_stringIdx \u003d  StringIndexer(inputCol\u003d\"income\", outputCol\u003d\"newlabel\")\nstages +\u003d [label_stringIdx]"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nassemblerInputs \u003d [c + \"classVec\" for c in CATE_FEATURES] + CONTI_FEATURES\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nassembler \u003d VectorAssembler(inputCols\u003dassemblerInputs, outputCol\u003d\"features\")\nstages +\u003d [assembler]"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Create a Pipeline.\npipeline \u003d Pipeline(stages\u003dstages)\npipelineModel \u003d pipeline.fit(df_remove)\nmodel \u003d pipelineModel.transform(df_remove)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nmodel.take(1)"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.linalg import DenseVector\ninput_data \u003d model.rdd.map(lambda x: (x[\"newlabel\"], DenseVector(x[\"features\"])))"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_train \u003d sqlContext.createDataFrame(input_data, [\"income\", \"features\"])"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndf_train.show(2)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Split the data into train and test sets\ntrain_data, test_data \u003d df_train.randomSplit([.8,.2],seed\u003d1234)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntrain_data.groupby(\u0027income\u0027).agg({\u0027income\u0027: \u0027count\u0027}).show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ntest_data.groupby(\u0027income\u0027).agg({\u0027income\u0027: \u0027count\u0027}).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Import `LinearRegression`\nfrom pyspark.ml.classification import LogisticRegression\n# Initialize `lr`\nlr \u003d LogisticRegression(labelCol\u003d\"income\",\n                        featuresCol\u003d\"features\",\n                        maxIter\u003d10,\n                        regParam\u003d0.3)\n# Fit the data to the model\nlinearModel \u003d lr.fit(train_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Print the coefficients and intercept for logistic regression\nprint(\"Coefficients: \" + str(linearModel.coefficients))\nprint(\"Intercept: \" + str(linearModel.intercept))"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n# Make predictions on test data using the transform() method.\npredictions \u003d linearModel.transform(test_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\npredictions.printSchema()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nselected \u003d predictions.select(\"income\", \"prediction\", \"probability\")\nselected.show(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncm \u003d predictions.select(\"income\", \"prediction\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncm.groupby(\u0027income\u0027).agg({\u0027income\u0027: \u0027count\u0027}).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncm.groupby(\u0027prediction\u0027).agg({\u0027prediction\u0027: \u0027count\u0027}).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ncm.filter(cm.income \u003d\u003d cm.prediction).count() / cm.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\ndef accuracy_m(model): \n    predictions \u003d model.transform(test_data)\n    cm \u003d predictions.select(\"income\", \"prediction\")\n    acc \u003d cm.filter(cm.income \u003d\u003d cm.prediction).count() / cm.count()\n    print(\"Model accuracy: %.3f%%\" % (acc * 100)) \naccuracy_m(model \u003d linearModel)"
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\n### Use ROC \nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\n# Evaluate model\nevaluator \u003d BinaryClassificationEvaluator(rawPredictionCol\u003d\"rawPrediction\", labelCol\u003d\u0027income\u0027)\nprint(evaluator.evaluate(predictions))\nprint(evaluator.getMetricName())"
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n# Create ParamGrid for Cross Validation\nparamGrid \u003d (ParamGridBuilder()\n             .addGrid(lr.regParam, [0.01, 0.5])\n             .build())"
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\nfrom time import *\nstart_time \u003d time()\n# Create 5-fold CrossValidator\ncv \u003d CrossValidator(estimator\u003dlr,\n                    estimatorParamMaps\u003dparamGrid,\n                    evaluator\u003devaluator, numFolds\u003d5)\n# Run cross validations\ncvModel \u003d cv.fit(train_data)\n# likely take a fair amount of time\nend_time \u003d time()\nelapsed_time \u003d end_time - start_time\nprint(\"Time to train model: %.3f seconds\" % elapsed_time)"
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%pyspark\naccuracy_m(model \u003d cvModel)\n\n\nbestModel \u003d cvModel.bestModel\nbestModel.extractParamMap()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}